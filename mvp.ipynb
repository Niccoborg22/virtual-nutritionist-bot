{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path\n",
    "1. Create the model in GCP\n",
    "    - Create the Dialogflow structure\n",
    "    - Implement the PaLM into the Dialogflow\n",
    "    - Test it out \n",
    "2. Make the backend locally\n",
    "3. Create a Front End\n",
    "4. Deploy the Front End"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "- Add identification in the frontend and change the conversational model if the user is identified. For example in the welcome intent instead of saying the fir interaction, get it ready to do something\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Importing all the libraries needed to run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\borga\\AppData\\Local\\Temp\\ipykernel_19788\\1392095229.py\", line 4, in respond\n",
      "    bot_message = dialogflow_request(message, 'en')\n",
      "  File \"C:\\Users\\borga\\AppData\\Local\\Temp\\ipykernel_19788\\4037815423.py\", line 10, in dialogflow_request\n",
      "    llm_response = model.predict((response_message.split(\"<<<llmquestion:\")[1][:-3]), **parameters)\n",
      "TypeError: ObjectDetectionModel.predict() got an unexpected keyword argument 'temperature'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\borga\\AppData\\Local\\Temp\\ipykernel_19788\\1392095229.py\", line 4, in respond\n",
      "    bot_message = dialogflow_request(message, 'en')\n",
      "  File \"C:\\Users\\borga\\AppData\\Local\\Temp\\ipykernel_19788\\4037815423.py\", line 10, in dialogflow_request\n",
      "    llm_response = model.predict((response_message.split(\"<<<llmquestion:\")[1][:-3]), **parameters)\n",
      "TypeError: ObjectDetectionModel.predict() got an unexpected keyword argument 'temperature'\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import dialogflow\n",
    "import uuid\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent creation\n",
    "---\n",
    "**ADD IMAGES ONCE FINISHED**\n",
    "\n",
    "\n",
    "# Agent connection\n",
    "---\n",
    "In order to connect to the project created in GCP and tHe agent created in Dialogflow we will have to follow a series of steps starting from authentication\n",
    "### Authentication\n",
    "Authentication as a service account in the Google Cloud Platform to access the resources in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the computer as a service account\n",
    "service_account_key = os.getenv('service_account_key')\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Creation\n",
    "A session represents a conversation between a Dialogflow agent and an end-user. You create a session at the beginning of a conversation and use it for each turn of the conversation. Once the conversation has ended, you discontinue using the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the variables to access the databases\n",
    "project_id = os.getenv('project_id')\n",
    "language_code = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the session with the appropriate credentials\n",
    "session_client = dialogflow.SessionsClient(credentials=credentials)\n",
    "\n",
    "session = session_client.session_path(project_id, session_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = os.getenv('region')\n",
    "aiplatform.init(project=project_id, location=region)\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of text model\n",
    "parameters = {\n",
    "    \"temperature\": 0.2,  # Temperature controls the degree of randomness in token selection.\n",
    "    \"max_output_tokens\": 1024,  # Token limit determines the maximum amount of text output.\n",
    "    \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "    \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "}\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "# response = model.predict('What are nutrients?', **parameters)\n",
    "# print(f\"Response from Model: {response.text}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent function\n",
    "Function that given an input returns the output of by connecting to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request an answer to our dialogflow agent according to a text\n",
    "def dialogflow_request(text, language_code):\n",
    "    text_input = dialogflow.TextInput(text=text, language_code=language_code)\n",
    "    query_input = dialogflow.QueryInput(text=text_input)\n",
    "    response = session_client.detect_intent(\n",
    "        request={\"session\": session, \"query_input\": query_input}\n",
    "    )\n",
    "    response_message = str(response.query_result.fulfillment_messages[0].text.text[0])\n",
    "    if \"<<<llmquestion\" in response_message:\n",
    "        llm_response = model.predict((response_message.split(\"<<<llmquestion:\")[1][:-3]), **parameters)\n",
    "        return llm_response.text\n",
    "    else:\n",
    "        return response_message\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roboflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20  Python-3.10.9 torch-2.0.1+cpu CPU\n",
      "Setup complete  (8 CPUs, 7.8 GB RAM, 310.3/475.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API key \n",
    "api_key = os.getenv('roboflow_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Set up the model\n",
    "rf = Roboflow(api_key=api_key)\n",
    "project = rf.workspace().project(\"nutrition-object-detection\")\n",
    "model = project.version(1).model\n",
    "\n",
    "results = model.predict(\"tryout.jpeg\", confidence=50, overlap=50).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_recognition(imagePath):\n",
    "    # infer on a local image\n",
    "    results = model.predict(imagePath, confidence=50, overlap=50).json()\n",
    "    if results['predictions'] == []:\n",
    "        prediction = 'Not found'\n",
    "    else:\n",
    "        prediction = results['predictions'][0]['class']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Application \n",
    "---\n",
    "Web application using Gradio to interact with the agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Images\n",
    "The user will hav the opportunity of inputting images to the chatbot. In order to do so the following functions are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get an answer from Dialogflow according to an input\n",
    "def respond(message, chat_history):\n",
    "    # Use the dialogflow function to get the answer\n",
    "    bot_message = dialogflow_request(message, 'en')\n",
    "    chat_history.append((message, bot_message))\n",
    "    time.sleep(1)\n",
    "    return \"\", chat_history\n",
    "\n",
    "# Function to add text to the chat\n",
    "def add_text(history, text):\n",
    "    history.append((text, None))\n",
    "    return history, respond(text)\n",
    "\n",
    "# Function to add a file\n",
    "def add_file(history, file):\n",
    "    history.append(((file.name,), None))\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "You have unused kwarg parameters in Row, please remove them: {'scale': 0.7}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\routes.py\", line 437, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\blocks.py\", line 1352, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\gradio\\blocks.py\", line 1077, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\borga\\Desktop\\IE Computer Science\\Semester 3\\NLP Conversational Model\\Final Project\\myenv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\borga\\AppData\\Local\\Temp\\ipykernel_19788\\2669466048.py\", line 10, in respond\n",
      "    bot_message = dialogflow_request(message, 'en')\n",
      "  File \"C:\\Users\\borga\\AppData\\Local\\Temp\\ipykernel_19788\\4037815423.py\", line 10, in dialogflow_request\n",
      "    llm_response = model.predict((response_message.split(\"<<<llmquestion:\")[1][:-3]), **parameters)\n",
      "TypeError: ObjectDetectionModel.predict() got an unexpected keyword argument 'temperature'\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(css=\".gradio-container {background-color: DarkSlateBlue}\") as demo:\n",
    "    disclaimer_agreed = gr.State(False)\n",
    "    def conditions_met(newValue):\n",
    "        disclaimer_agreed.value = newValue\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "            <h1 style=\"text-align: center; color:white;\" >FitBot</h1>\n",
    "            <h4 style=\"text-align: center; color:white\">Your personal food and health assistant</h4>\n",
    "            \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            # Create a chatbot component and a model component\n",
    "            chatbot = gr.Chatbot([], elem_id=\"chatbot\", label=\"FitBot\").style()\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"\"\"\n",
    "            <h2 style=\"color:white\" >Disclaimer</h2>\n",
    "            <p style=\"color:white\">The following bot is not meant following any regulatory health practice. All the information that is presented to you\n",
    "            is generated from Vertex AI, the generative AI developed by Google. If you are interested in knowing more about the project and how this model\n",
    "            was built, please refer to the following <a href=\"https://github.com/Niccoborg22/virtual-nutritionist-bot\" style=\"color:white\">Github link</a></p>\n",
    "            \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Create a textbox component\n",
    "        with gr.Column(scale=8):\n",
    "            msg = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"Enter text and press enter, or upload an image\",\n",
    "            ).style(container=False)\n",
    "        with gr.Column(scale=1, min_width=0):\n",
    "            # Create an upload button component to add images\n",
    "            btn = gr.UploadButton(\"📁\", file_types=[\"image\"])\n",
    "\n",
    "    with gr.Row(scale=0.7):\n",
    "        clear_btn = gr.Button(value=\"🗑️  Clear\")\n",
    "\n",
    "    \n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        \n",
    "    # Create a message component for file uploads\n",
    "    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False)\n",
    "    clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# launch the application\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
